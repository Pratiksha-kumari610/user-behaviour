# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oe3OyQTMCPpiYc7hjpOURHuiaG1jfh5l
"""

import nltk
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')
!pip install emoji contractions

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import contractions
import emoji
import re

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier
from sklearn.svm import SVC
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

df=pd.read_csv('/content/twitter_training.csv.zip')
df.head()

df.shape

columns=["ID","ENTITY","Label","Text"]
df=pd.read_csv('/content/twitter_training.csv.zip',names=columns)
df.head()

df['Label'].value_counts()

df_train = df[df['Label'] != 'Irrelevant']

df_train.isna().sum()

df_train.dropna(inplace=True)

df_train.duplicated().sum()

df_train.drop_duplicates()

df_train.isna().sum()

x=df_train["ENTITY"].value_counts()
x

y=df_train["Label"].value_counts()
y

plt.figure(figsize=(5,5))
sns.barplot(x=y.index,y=y.values)
plt.xlabel(" Label of Text ")
plt.ylabel(" Distribution ")
plt.show()

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'@\w+', '', text)
    text = emoji.demojize(text, delimiters=(" ", " "))
    text = contractions.fix(text)
    text = re.sub(r'\s+', ' ', text).strip()

    tokens = word_tokenize(text)

    negations = {"not", "no", "nor", "never"}
    stop_words = set(stopwords.words('english')) - negations
    tokens = [word for word in tokens if word not in stop_words]

    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]

    return ' '.join(tokens)

# Apply to your dataframe
df_train['processed_text'] = df_train['Text'].apply(preprocess_text)
df_train[['Text', 'processed_text']].head()

df_train

encoder=LabelEncoder()
df_train["Label"]=encoder.fit_transform(df_train['Label'])
df_train

df_train['Label'].value_counts()

x=df_train["processed_text"]
y= df_train['Label']
X_train, X_test, y_train, y_test = train_test_split(x,y ,test_size=0.2, random_state=42, stratify=y)

print(X_train.shape)
print(X_test.shape)

vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,3))
X_train_vectorized=vectorizer.fit_transform(X_train)
X_test_vectorized=vectorizer.transform(X_test)

print(X_train_vectorized.shape)
print(X_test_vectorized.shape)

from google.colab import drive
drive.mount('/content/drive')